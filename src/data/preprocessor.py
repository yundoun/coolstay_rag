"""
CoolStay RAG ì‹œìŠ¤í…œ ë¬¸ì„œ ì „ì²˜ë¦¬ ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œì˜ ì „ì²˜ë¦¬ ë° ì •ê·œí™” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import re
import logging
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass

from langchain_core.documents import Document

logger = logging.getLogger(__name__)


@dataclass
class PreprocessingStats:
    """ì „ì²˜ë¦¬ í†µê³„"""
    original_length: int
    processed_length: int
    lines_removed: int
    characters_cleaned: int
    empty_lines_removed: int
    code_blocks_processed: int
    links_processed: int
    headers_normalized: int


class MarkdownPreprocessor:
    """ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œ ì „ì²˜ë¦¬ í´ë˜ìŠ¤"""

    def __init__(self):
        # ì •ë¦¬í•  íŒ¨í„´ë“¤
        self.cleanup_patterns = {
            # ê³¼ë„í•œ ê³µë°± ë° ì¤„ë°”ê¿ˆ
            'excessive_newlines': re.compile(r'\n{4,}'),
            'excessive_spaces': re.compile(r'[ \t]{3,}'),
            'trailing_whitespace': re.compile(r'[ \t]+$', re.MULTILINE),

            # ì˜ëª»ëœ ë§ˆí¬ë‹¤ìš´ ë¬¸ë²•
            'invalid_headers': re.compile(r'^#{7,}', re.MULTILINE),
            'empty_headers': re.compile(r'^#+\s*$', re.MULTILINE),

            # íŠ¹ìˆ˜ ë¬¸ì ì •ë¦¬
            'weird_quotes': re.compile(r'[""''â€â€šâ€¹â€ºÂ«Â»]'),
            'weird_dashes': re.compile(r'[â€“â€”â€•]'),
            'weird_apostrophes': re.compile(r'[''â€š]'),

            # HTML íƒœê·¸ (ê¸°ë³¸ì ì¸ ê²ƒë§Œ)
            'html_comments': re.compile(r'<!--.*?-->', re.DOTALL),
            'html_tags': re.compile(r'<[^>]+>'),
        }

        # ì •ê·œí™” ê·œì¹™
        self.normalization_rules = {
            'quotes': (r'[""''â€â€š]', '"'),
            'apostrophes': (r'[''â€š]', "'"),
            'dashes': (r'[â€“â€”â€•]', '-'),
            'ellipsis': (r'\.{3,}', '...'),
        }

    def clean_text(self, text: str) -> Tuple[str, Dict[str, int]]:
        """í…ìŠ¤íŠ¸ ê¸°ë³¸ ì •ë¦¬"""
        stats = {
            'characters_removed': 0,
            'lines_removed': 0,
            'patterns_cleaned': 0
        }

        original_length = len(text)
        original_lines = text.count('\n')

        # 1. HTML ì£¼ì„ ë° íƒœê·¸ ì œê±°
        text = self.cleanup_patterns['html_comments'].sub('', text)
        text = self.cleanup_patterns['html_tags'].sub('', text)

        # 2. ì˜ëª»ëœ í—¤ë” ìˆ˜ì •
        text = self.cleanup_patterns['invalid_headers'].sub('###### ', text)
        text = self.cleanup_patterns['empty_headers'].sub('', text)

        # 3. ê³µë°± ì •ë¦¬
        text = self.cleanup_patterns['trailing_whitespace'].sub('', text)
        text = self.cleanup_patterns['excessive_spaces'].sub('  ', text)
        text = self.cleanup_patterns['excessive_newlines'].sub('\n\n\n', text)

        # í†µê³„ ê³„ì‚°
        stats['characters_removed'] = original_length - len(text)
        stats['lines_removed'] = original_lines - text.count('\n')

        return text, stats

    def normalize_unicode(self, text: str) -> str:
        """ìœ ë‹ˆì½”ë“œ ë¬¸ì ì •ê·œí™”"""
        for pattern, replacement in self.normalization_rules.values():
            text = re.sub(pattern, replacement, text)
        return text

    def fix_markdown_syntax(self, text: str) -> Tuple[str, int]:
        """ë§ˆí¬ë‹¤ìš´ ë¬¸ë²• ìˆ˜ì •"""
        fixes_count = 0

        # í—¤ë” ê³µë°± ìˆ˜ì •
        text = re.sub(r'^(#+)([^\s])', r'\1 \2', text, flags=re.MULTILINE)
        fixes_count += len(re.findall(r'^(#+)([^\s])', text, flags=re.MULTILINE))

        # ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ ê³µë°± ìˆ˜ì •
        text = re.sub(r'^([-*+])([^\s])', r'\1 \2', text, flags=re.MULTILINE)
        text = re.sub(r'^(\d+\.)([^\s])', r'\1 \2', text, flags=re.MULTILINE)

        # ë§í¬ ë¬¸ë²• ìˆ˜ì •
        text = re.sub(r'\[([^\]]+)\]\s+\(([^)]+)\)', r'[\1](\2)', text)

        # ì½”ë“œ ë¸”ë¡ ì–¸ì–´ í‘œì‹œ ì •ë¦¬
        text = re.sub(r'^```\s*$', '```', text, flags=re.MULTILINE)

        return text, fixes_count

    def remove_empty_sections(self, text: str) -> Tuple[str, int]:
        """ë¹ˆ ì„¹ì…˜ ì œê±°"""
        # ì—°ì†ëœ í—¤ë” (ë‚´ìš©ì´ ì—†ëŠ” ì„¹ì…˜) ì œê±°
        empty_sections_pattern = r'^(#+\s+[^\n]*)\n+(?=^#+\s+|\Z)'
        matches = re.findall(empty_sections_pattern, text, flags=re.MULTILINE)
        text = re.sub(empty_sections_pattern, '', text, flags=re.MULTILINE)

        return text, len(matches)

    def enhance_code_blocks(self, text: str) -> Tuple[str, int]:
        """ì½”ë“œ ë¸”ë¡ ê°œì„ """
        code_blocks_processed = 0

        # ì½”ë“œ ë¸”ë¡ì— ì–¸ì–´ ì§€ì •ì´ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬
        def add_language_hint(match):
            nonlocal code_blocks_processed
            code_content = match.group(1)

            # ê°„ë‹¨í•œ ì–¸ì–´ ê°ì§€
            if re.search(r'\b(import|export|const|let|var|function)\b', code_content):
                language = 'javascript'
            elif re.search(r'\b(def|import|class|if __name__)\b', code_content):
                language = 'python'
            elif re.search(r'\b(SELECT|FROM|WHERE|INSERT)\b', code_content.upper()):
                language = 'sql'
            elif re.search(r'<[^>]+>', code_content):
                language = 'html'
            else:
                language = ''

            code_blocks_processed += 1
            return f'```{language}\n{code_content}```'

        text = re.sub(r'```\n(.*?)```', add_language_hint, text, flags=re.DOTALL)

        return text, code_blocks_processed

    def standardize_headers(self, text: str) -> Tuple[str, int]:
        """í—¤ë” í‘œì¤€í™”"""
        headers_normalized = 0

        # í—¤ë” ë ˆë²¨ í‘œì¤€í™” (ìµœëŒ€ 4ë ˆë²¨ê¹Œì§€ë§Œ)
        lines = text.split('\n')
        normalized_lines = []

        for line in lines:
            if re.match(r'^#+', line):
                # í—¤ë” ë ˆë²¨ ê³„ì‚°
                header_match = re.match(r'^(#+)\s*(.*)', line)
                if header_match:
                    level = min(len(header_match.group(1)), 4)  # ìµœëŒ€ 4ë ˆë²¨
                    content = header_match.group(2).strip()

                    if content:  # ë‚´ìš©ì´ ìˆëŠ” í—¤ë”ë§Œ ìœ ì§€
                        normalized_line = '#' * level + ' ' + content
                        normalized_lines.append(normalized_line)
                        headers_normalized += 1
                    continue

            normalized_lines.append(line)

        return '\n'.join(normalized_lines), headers_normalized

    def process_links(self, text: str) -> Tuple[str, int]:
        """ë§í¬ ì²˜ë¦¬ ë° ì •ë¦¬"""
        links_processed = 0

        # ìƒëŒ€ê²½ë¡œë¥¼ ì ˆëŒ€ê²½ë¡œë¡œ ë³€í™˜í•˜ê±°ë‚˜ ì œê±°
        def process_link(match):
            nonlocal links_processed
            link_text = match.group(1)
            url = match.group(2)

            # ë‚´ë¶€ ë§í¬ë‚˜ ì•µì»¤ëŠ” ì œê±°
            if url.startswith('#') or url.startswith('./') or url.startswith('../'):
                links_processed += 1
                return link_text

            # HTTP(S) ë§í¬ëŠ” ìœ ì§€
            if url.startswith(('http://', 'https://')):
                return match.group(0)

            # ë‚˜ë¨¸ì§€ëŠ” í…ìŠ¤íŠ¸ë§Œ ìœ ì§€
            links_processed += 1
            return link_text

        text = re.sub(r'\[([^\]]+)\]\(([^)]+)\)', process_link, text)

        return text, links_processed

    def preprocess_content(self, content: str, domain: str = "unknown") -> Tuple[str, PreprocessingStats]:
        """ì „ì²´ ì „ì²˜ë¦¬ í”„ë¡œì„¸ìŠ¤"""
        original_length = len(content)

        # í†µê³„ ëˆ„ì 
        total_stats = PreprocessingStats(
            original_length=original_length,
            processed_length=0,
            lines_removed=0,
            characters_cleaned=0,
            empty_lines_removed=0,
            code_blocks_processed=0,
            links_processed=0,
            headers_normalized=0
        )

        try:
            # 1. ê¸°ë³¸ í…ìŠ¤íŠ¸ ì •ë¦¬
            content, clean_stats = self.clean_text(content)
            total_stats.characters_cleaned = clean_stats['characters_removed']
            total_stats.lines_removed = clean_stats['lines_removed']

            # 2. ìœ ë‹ˆì½”ë“œ ì •ê·œí™”
            content = self.normalize_unicode(content)

            # 3. ë§ˆí¬ë‹¤ìš´ ë¬¸ë²• ìˆ˜ì •
            content, syntax_fixes = self.fix_markdown_syntax(content)

            # 4. ë¹ˆ ì„¹ì…˜ ì œê±°
            content, empty_sections = self.remove_empty_sections(content)
            total_stats.empty_lines_removed = empty_sections

            # 5. ì½”ë“œ ë¸”ë¡ ê°œì„ 
            content, code_blocks = self.enhance_code_blocks(content)
            total_stats.code_blocks_processed = code_blocks

            # 6. í—¤ë” í‘œì¤€í™”
            content, headers = self.standardize_headers(content)
            total_stats.headers_normalized = headers

            # 7. ë§í¬ ì²˜ë¦¬
            content, links = self.process_links(content)
            total_stats.links_processed = links

            # 8. ìµœì¢… ì •ë¦¬
            content = content.strip()
            total_stats.processed_length = len(content)

            logger.info(f"âœ… {domain} ì „ì²˜ë¦¬ ì™„ë£Œ: {original_length:,} â†’ {len(content):,}ì")

            return content, total_stats

        except Exception as e:
            logger.error(f"ì „ì²˜ë¦¬ ì‹¤íŒ¨ {domain}: {e}")
            return content, total_stats

    def preprocess_document(self, document: Document) -> Document:
        """Document ê°ì²´ ì „ì²˜ë¦¬"""
        domain = document.metadata.get('domain', 'unknown')

        processed_content, stats = self.preprocess_content(document.page_content, domain)

        # ì „ì²˜ë¦¬ í†µê³„ë¥¼ ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€
        processed_metadata = {
            **document.metadata,
            'preprocessing': {
                'original_length': stats.original_length,
                'processed_length': stats.processed_length,
                'reduction_ratio': (stats.original_length - stats.processed_length) / stats.original_length if stats.original_length > 0 else 0,
                'characters_cleaned': stats.characters_cleaned,
                'lines_removed': stats.lines_removed,
                'empty_lines_removed': stats.empty_lines_removed,
                'code_blocks_processed': stats.code_blocks_processed,
                'links_processed': stats.links_processed,
                'headers_normalized': stats.headers_normalized,
            }
        }

        return Document(
            page_content=processed_content,
            metadata=processed_metadata
        )

    def preprocess_documents(self, documents: List[Document]) -> List[Document]:
        """ì—¬ëŸ¬ Document ê°ì²´ ì¼ê´„ ì „ì²˜ë¦¬"""
        processed_documents = []

        for i, doc in enumerate(documents):
            try:
                processed_doc = self.preprocess_document(doc)
                processed_documents.append(processed_doc)
            except Exception as e:
                logger.error(f"ë¬¸ì„œ {i} ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                # ì›ë³¸ ë¬¸ì„œ ìœ ì§€
                processed_documents.append(doc)

        logger.info(f"âœ… {len(processed_documents)}/{len(documents)}ê°œ ë¬¸ì„œ ì „ì²˜ë¦¬ ì™„ë£Œ")
        return processed_documents

    def validate_processed_content(self, content: str) -> Dict[str, Any]:
        """ì „ì²˜ë¦¬ëœ ë‚´ìš© ê²€ì¦"""
        validation = {
            'valid': True,
            'warnings': [],
            'issues': []
        }

        # ê¸°ë³¸ ê²€ì¦
        if not content.strip():
            validation['valid'] = False
            validation['issues'].append('ë¹ˆ ë‚´ìš©')
            return validation

        # ìµœì†Œ ê¸¸ì´ ê²€ì¦
        if len(content) < 50:
            validation['warnings'].append('ë§¤ìš° ì§§ì€ ë‚´ìš© (50ì ë¯¸ë§Œ)')

        # ë§ˆí¬ë‹¤ìš´ ë¬¸ë²• ê²€ì¦
        if not re.search(r'^#+\s+', content, re.MULTILINE):
            validation['warnings'].append('í—¤ë”ê°€ ì—†ìŒ')

        # ë¶ˆì™„ì „í•œ ì½”ë“œ ë¸”ë¡ ê²€ì¦
        code_block_count = content.count('```')
        if code_block_count % 2 != 0:
            validation['issues'].append('ë¶ˆì™„ì „í•œ ì½”ë“œ ë¸”ë¡')
            validation['valid'] = False

        # ê³¼ë„í•œ ì¤‘ë³µ íŒ¨í„´ ê²€ì¦
        lines = content.split('\n')
        unique_lines = set(line.strip() for line in lines if line.strip())
        if len(lines) > 10 and len(unique_lines) / len(lines) < 0.5:
            validation['warnings'].append('ê³¼ë„í•œ ì¤‘ë³µ ë‚´ìš©')

        return validation

    def get_preprocessing_summary(self, stats_list: List[PreprocessingStats]) -> Dict[str, Any]:
        """ì „ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½"""
        if not stats_list:
            return {}

        total_original = sum(s.original_length for s in stats_list)
        total_processed = sum(s.processed_length for s in stats_list)
        total_cleaned = sum(s.characters_cleaned for s in stats_list)

        return {
            'total_documents': len(stats_list),
            'total_original_length': total_original,
            'total_processed_length': total_processed,
            'total_reduction': total_original - total_processed,
            'avg_reduction_ratio': (total_original - total_processed) / total_original if total_original > 0 else 0,
            'total_characters_cleaned': total_cleaned,
            'total_lines_removed': sum(s.lines_removed for s in stats_list),
            'total_empty_sections_removed': sum(s.empty_lines_removed for s in stats_list),
            'total_code_blocks_processed': sum(s.code_blocks_processed for s in stats_list),
            'total_links_processed': sum(s.links_processed for s in stats_list),
            'total_headers_normalized': sum(s.headers_normalized for s in stats_list),
        }


# í¸ì˜ í•¨ìˆ˜ë“¤
def preprocess_text(content: str, domain: str = "unknown") -> str:
    """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í¸ì˜ í•¨ìˆ˜"""
    preprocessor = MarkdownPreprocessor()
    processed_content, _ = preprocessor.preprocess_content(content, domain)
    return processed_content


def preprocess_document(document: Document) -> Document:
    """Document ì „ì²˜ë¦¬ í¸ì˜ í•¨ìˆ˜"""
    preprocessor = MarkdownPreprocessor()
    return preprocessor.preprocess_document(document)


def preprocess_documents(documents: List[Document]) -> List[Document]:
    """Documents ì¼ê´„ ì „ì²˜ë¦¬ í¸ì˜ í•¨ìˆ˜"""
    preprocessor = MarkdownPreprocessor()
    return preprocessor.preprocess_documents(documents)


def validate_content(content: str) -> Dict[str, Any]:
    """ë‚´ìš© ê²€ì¦ í¸ì˜ í•¨ìˆ˜"""
    preprocessor = MarkdownPreprocessor()
    return preprocessor.validate_processed_content(content)


if __name__ == "__main__":
    # ì „ì²˜ë¦¬ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸
    print("ğŸ”§ CoolStay ì „ì²˜ë¦¬ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸")
    print("=" * 50)

    # í…ŒìŠ¤íŠ¸ ë§ˆí¬ë‹¤ìš´ ë‚´ìš©
    test_content = """
#  í…ŒìŠ¤íŠ¸ ë¬¸ì„œ

##ì„¹ì…˜ 1
ì´ê²ƒì€    ì²« ë²ˆì§¸   ì„¹ì…˜ì…ë‹ˆë‹¤.
ì—¬ê¸°ì—ëŠ” "ì´ìƒí•œ" ë”°ì˜´í‘œì™€ â€“ëŒ€ì‹œâ€“ ê·¸ë¦¬ê³  'apostrophe'ê°€ ìˆìŠµë‹ˆë‹¤.



### í•˜ìœ„ ì„¹ì…˜ 1.1
```
console.log("Hello World");
```

##

## ì„¹ì…˜ 2
ë‘ ë²ˆì§¸ ì„¹ì…˜ì…ë‹ˆë‹¤.

<div>HTML íƒœê·¸</div>

<!-- HTML ì£¼ì„ -->

[ë§í¬](./local-file.md)

[ì™¸ë¶€ë§í¬](https://example.com)
"""

    preprocessor = MarkdownPreprocessor()

    print("ğŸ“ ì›ë³¸ ë‚´ìš©:")
    print(f"   ê¸¸ì´: {len(test_content)}ì")
    print(f"   ì¤„ ìˆ˜: {test_content.count(chr(10))}ì¤„")

    # ì „ì²˜ë¦¬ ì‹¤í–‰
    processed_content, stats = preprocessor.preprocess_content(test_content, "test")

    print(f"\nâœ¨ ì „ì²˜ë¦¬ ê²°ê³¼:")
    print(f"   ê¸¸ì´: {stats.processed_length}ì (ì›ë³¸ ëŒ€ë¹„ {stats.processed_length/stats.original_length*100:.1f}%)")
    print(f"   ì •ë¦¬ëœ ë¬¸ì: {stats.characters_cleaned}ê°œ")
    print(f"   ì œê±°ëœ ì¤„: {stats.lines_removed}ê°œ")
    print(f"   ì²˜ë¦¬ëœ ì½”ë“œë¸”ë¡: {stats.code_blocks_processed}ê°œ")
    print(f"   ì²˜ë¦¬ëœ ë§í¬: {stats.links_processed}ê°œ")
    print(f"   ì •ê·œí™”ëœ í—¤ë”: {stats.headers_normalized}ê°œ")

    # ê²€ì¦
    validation = preprocessor.validate_processed_content(processed_content)
    print(f"\nğŸ” ê²€ì¦ ê²°ê³¼: {'âœ… ìœ íš¨' if validation['valid'] else 'âŒ ë¬¸ì œ'}")

    if validation['warnings']:
        print("   âš ï¸ ê²½ê³ :")
        for warning in validation['warnings']:
            print(f"      - {warning}")

    if validation['issues']:
        print("   âŒ ë¬¸ì œ:")
        for issue in validation['issues']:
            print(f"      - {issue}")

    print(f"\nğŸ“„ ì „ì²˜ë¦¬ëœ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°:")
    print("-" * 30)
    print(processed_content[:500] + "..." if len(processed_content) > 500 else processed_content)