"""
CoolStay RAG ì‹œìŠ¤í…œ ì§ˆë¬¸ ë¶„ì„ ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ë„ë©”ì¸ê³¼ ì—ì´ì „íŠ¸ë¥¼ ì„ íƒí•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import logging
import re
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser

from ..core.config import CoolStayConfig

from ..core.config import config, get_domain_config
from ..core.llm import CoolStayLLM, get_default_llm

logger = logging.getLogger(__name__)


class QuestionType(Enum):
    """ì§ˆë¬¸ ìœ í˜•"""
    SPECIFIC_DOMAIN = "specific_domain"      # íŠ¹ì • ë„ë©”ì¸ ì§ˆë¬¸
    MULTI_DOMAIN = "multi_domain"           # ë‹¤ì¤‘ ë„ë©”ì¸ ì§ˆë¬¸
    GENERAL = "general"                     # ì¼ë°˜ì ì¸ ì§ˆë¬¸
    WEB_SEARCH = "web_search"              # ì›¹ ê²€ìƒ‰ í•„ìš”
    UNCLEAR = "unclear"                     # ë¶ˆë¶„ëª…í•œ ì§ˆë¬¸


class UrgencyLevel(Enum):
    """ê¸´ê¸‰ë„ ë ˆë²¨"""
    HIGH = "high"       # ë†’ìŒ (ì¦‰ì‹œ ì²˜ë¦¬)
    MEDIUM = "medium"   # ë³´í†µ (ì¼ë°˜ ì²˜ë¦¬)
    LOW = "low"         # ë‚®ìŒ (ì§€ì—° ì²˜ë¦¬ ê°€ëŠ¥)


@dataclass
class QuestionAnalysis:
    """ì§ˆë¬¸ ë¶„ì„ ê²°ê³¼"""
    question: str
    question_type: QuestionType
    primary_domains: List[str]              # ì£¼ìš” ê´€ë ¨ ë„ë©”ì¸ë“¤
    secondary_domains: List[str]            # ë¶€ì°¨ì  ê´€ë ¨ ë„ë©”ì¸ë“¤
    confidence_score: float                 # ë¶„ì„ í™•ì‹ ë„ (0-1)
    urgency_level: UrgencyLevel
    keywords: List[str]                     # ì¶”ì¶œëœ í‚¤ì›Œë“œ
    intent: str                            # ì§ˆë¬¸ ì˜ë„
    complexity: str                        # ë³µì¡ë„ (simple/medium/complex)
    requires_web_search: bool              # ì›¹ ê²€ìƒ‰ í•„ìš” ì—¬ë¶€
    reasoning: str                         # ë¶„ì„ ê·¼ê±°
    metadata: Optional[Dict[str, Any]] = None


class QuestionAnalyzer:
    """ì§ˆë¬¸ ë¶„ì„ê¸°"""

    def __init__(self, llm: Optional[CoolStayLLM] = None):
        """
        ì§ˆë¬¸ ë¶„ì„ê¸° ì´ˆê¸°í™”

        Args:
            llm: ì‚¬ìš©í•  LLM ì¸ìŠ¤í„´ìŠ¤
        """
        self.llm = llm or get_default_llm()
        self.domain_info = self._prepare_domain_info()
        self._setup_analysis_prompt()

    def _prepare_domain_info(self) -> Dict[str, Dict[str, Any]]:
        """ë„ë©”ì¸ ì •ë³´ ì¤€ë¹„"""
        domain_info = {}

        for domain in config.domain_list:
            try:
                domain_config = get_domain_config(domain)
                domain_info[domain] = {
                    'description': domain_config.description,
                    'keywords': domain_config.keywords
                }
            except ValueError:
                logger.warning(f"ë„ë©”ì¸ ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {domain}")

        # ì›¹ ê²€ìƒ‰ ë„ë©”ì¸ ì¶”ê°€
        domain_info['web_search'] = config.web_search_config

        return domain_info

    def _setup_analysis_prompt(self):
        """ì§ˆë¬¸ ë¶„ì„ í”„ë¡¬í”„íŠ¸ ì„¤ì •"""
        # ë„ë©”ì¸ ì •ë³´ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        domain_descriptions = []
        for domain, info in self.domain_info.items():
            keywords = ', '.join(info['keywords']) if info['keywords'] else 'ì—†ìŒ'
            domain_descriptions.append(f"- **{domain}**: {info['description']} (í‚¤ì›Œë“œ: {keywords})")

        domain_info_text = '\n'.join(domain_descriptions)

        self.analysis_prompt = ChatPromptTemplate.from_template(f"""
ë‹¹ì‹ ì€ ê¿€ìŠ¤í…Œì´ RAG ì‹œìŠ¤í…œì˜ ì§ˆë¬¸ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ë„ë©”ì¸ê³¼ ì²˜ë¦¬ ë°©ë²•ì„ ê²°ì •í•´ì•¼ í•©ë‹ˆë‹¤.

**ì‚¬ìš© ê°€ëŠ¥í•œ ë„ë©”ì¸:**
{domain_info_text}

**ë¶„ì„í•  ì§ˆë¬¸:** {{question}}

**ë¶„ì„ ì§€ì¹¨:**
1. **ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜:**
   - specific_domain: íŠ¹ì • ë„ë©”ì¸ì—ë§Œ ê´€ë ¨ëœ ì§ˆë¬¸
   - multi_domain: ì—¬ëŸ¬ ë„ë©”ì¸ì— ê±¸ì¹œ ë³µí•© ì§ˆë¬¸
   - general: ë„ë©”ì¸ êµ¬ë¶„ì´ ëª…í™•í•˜ì§€ ì•Šì€ ì¼ë°˜ ì§ˆë¬¸
   - web_search: ìµœì‹  ì •ë³´ë‚˜ ì™¸ë¶€ ì •ë³´ê°€ í•„ìš”í•œ ì§ˆë¬¸
   - unclear: ì§ˆë¬¸ ì˜ë„ê°€ ë¶ˆë¶„ëª…í•œ ì§ˆë¬¸

2. **ë„ë©”ì¸ ì„ íƒ:**
   - primary_domains: ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ ë„ë©”ì¸ë“¤ (1-3ê°œ)
   - secondary_domains: ë¶€ì°¨ì ìœ¼ë¡œ ê´€ë ¨ëœ ë„ë©”ì¸ë“¤ (0-2ê°œ)
   - í‚¤ì›Œë“œ ë§¤ì¹­ê³¼ ì˜ë¯¸ì  ìœ ì‚¬ì„±ì„ ëª¨ë‘ ê³ ë ¤

3. **ì›¹ ê²€ìƒ‰ í•„ìš”ì„± íŒë‹¨:**
   - "ìµœì‹ ", "í˜„ì¬", "2024", "ë‰´ìŠ¤", "ë™í–¥" ë“±ì˜ í‚¤ì›Œë“œ
   - ì‹œê°„ ë¯¼ê°ì  ì •ë³´ ìš”êµ¬
   - ë‚´ë¶€ ë¬¸ì„œì— ì—†ì„ ê²ƒ ê°™ì€ ì™¸ë¶€ ì •ë³´

4. **ê¸´ê¸‰ë„ í‰ê°€:**
   - high: ì—…ë¬´ ì°¨ë‹¨, ê¸´ê¸‰ ì²˜ë¦¬ í•„ìš”
   - medium: ì¼ë°˜ì ì¸ ì—…ë¬´ ë¬¸ì˜
   - low: ì°¸ê³ ìš©, êµìœ¡ìš© ì§ˆë¬¸

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
    "question_type": "specific_domain|multi_domain|general|web_search|unclear",
    "primary_domains": ["domain1", "domain2"],
    "secondary_domains": ["domain3"],
    "confidence_score": 0.0-1.0,
    "urgency_level": "high|medium|low",
    "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", "í‚¤ì›Œë“œ3"],
    "intent": "ì§ˆë¬¸ì˜ í•µì‹¬ ì˜ë„ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ",
    "complexity": "simple|medium|complex",
    "requires_web_search": true|false,
    "reasoning": "ë¶„ì„ ê·¼ê±°ë¥¼ ìƒì„¸íˆ ì„¤ëª…"
}}
""")

        self.analysis_chain = (
            self.analysis_prompt
            | self.llm.llm
            | JsonOutputParser()
        )

    def analyze_question(self, question: str) -> QuestionAnalysis:
        """ì§ˆë¬¸ ë¶„ì„ ìˆ˜í–‰"""
        try:
            # LLMì„ í†µí•œ ê³ ê¸‰ ë¶„ì„
            result = self.analysis_chain.invoke({"question": question})

            # ê¸°ë³¸ê°’ ì²˜ë¦¬ ë° ê²€ì¦
            question_type = QuestionType(result.get("question_type", "general"))
            urgency_level = UrgencyLevel(result.get("urgency_level", "medium"))

            # ë„ë©”ì¸ ê²€ì¦ ë° ì •ë¦¬
            primary_domains = self._validate_domains(result.get("primary_domains", []))
            secondary_domains = self._validate_domains(result.get("secondary_domains", []))

            # ê·œì¹™ ê¸°ë°˜ ë³´ì™„ ë¶„ì„
            rule_based_analysis = self._rule_based_analysis(question)

            # ê²°ê³¼ í†µí•©
            final_analysis = QuestionAnalysis(
                question=question,
                question_type=question_type,
                primary_domains=primary_domains or rule_based_analysis['domains'],
                secondary_domains=secondary_domains,
                confidence_score=max(0.0, min(1.0, result.get("confidence_score", 0.7))),
                urgency_level=urgency_level,
                keywords=result.get("keywords", []) + rule_based_analysis['keywords'],
                intent=result.get("intent", "ì§ˆë¬¸ ë¶„ì„"),
                complexity=result.get("complexity", "medium"),
                requires_web_search=result.get("requires_web_search", False) or rule_based_analysis['web_search'],
                reasoning=result.get("reasoning", "LLM ê¸°ë°˜ ë¶„ì„"),
                metadata={
                    'llm_analysis': result,
                    'rule_based_analysis': rule_based_analysis,
                    'domain_count': len(primary_domains) + len(secondary_domains)
                }
            )

            logger.info(f"âœ… ì§ˆë¬¸ ë¶„ì„ ì™„ë£Œ: {question_type.value}, ë„ë©”ì¸ {len(primary_domains)}ê°œ")
            return final_analysis

        except Exception as e:
            logger.error(f"ì§ˆë¬¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
            # í´ë°±: ê·œì¹™ ê¸°ë°˜ ë¶„ì„ë§Œ ì‚¬ìš©
            return self._fallback_analysis(question)

    def _validate_domains(self, domains: List[str]) -> List[str]:
        """ë„ë©”ì¸ ëª©ë¡ ê²€ì¦"""
        valid_domains = []
        all_domains = list(config.domain_list) + ['web_search']

        for domain in domains:
            if domain in all_domains:
                valid_domains.append(domain)
            else:
                logger.warning(f"ì˜ëª»ëœ ë„ë©”ì¸ ë¬´ì‹œ: {domain}")

        return valid_domains

    def _rule_based_analysis(self, question: str) -> Dict[str, Any]:
        """ê·œì¹™ ê¸°ë°˜ ë³´ì™„ ë¶„ì„"""
        question_lower = question.lower()
        result = {
            'domains': [],
            'keywords': [],
            'web_search': False
        }

        # ë„ë©”ì¸ë³„ í‚¤ì›Œë“œ ë§¤ì¹­
        for domain, info in self.domain_info.items():
            if domain == 'web_search':
                continue

            matches = sum(1 for keyword in info['keywords']
                         if keyword.lower() in question_lower)

            if matches > 0:
                result['domains'].append(domain)
                # ë§¤ì¹­ëœ í‚¤ì›Œë“œ ì¶”ì¶œ
                matched_keywords = [kw for kw in info['keywords']
                                  if kw.lower() in question_lower]
                result['keywords'].extend(matched_keywords)

        # ì›¹ ê²€ìƒ‰ í•„ìš”ì„± íŒë‹¨
        web_indicators = [
            'ìµœì‹ ', 'í˜„ì¬', 'ì˜¤ëŠ˜', '2024', 'ë‰´ìŠ¤', 'ë™í–¥', 'íŠ¸ë Œë“œ',
            'ì—…ë°ì´íŠ¸', 'ë³€í™”', 'ë°œí‘œ', 'ì¶œì‹œ', 'latest', 'current', 'news'
        ]

        for indicator in web_indicators:
            if indicator in question_lower:
                result['web_search'] = True
                break

        # ê¸°ë³¸ ë„ë©”ì¸ (ì•„ë¬´ê²ƒë„ ë§¤ì¹­ë˜ì§€ ì•Šìœ¼ë©´)
        if not result['domains']:
            result['domains'] = ['business_policy']  # ê¸°ë³¸ì ìœ¼ë¡œ ë¹„ì¦ˆë‹ˆìŠ¤ ì •ì±…ìœ¼ë¡œ

        return result

    def _fallback_analysis(self, question: str) -> QuestionAnalysis:
        """í´ë°± ë¶„ì„ (LLM ë¶„ì„ ì‹¤íŒ¨ ì‹œ)"""
        rule_based = self._rule_based_analysis(question)

        return QuestionAnalysis(
            question=question,
            question_type=QuestionType.GENERAL,
            primary_domains=rule_based['domains'],
            secondary_domains=[],
            confidence_score=0.5,
            urgency_level=UrgencyLevel.MEDIUM,
            keywords=rule_based['keywords'],
            intent="ê·œì¹™ ê¸°ë°˜ ë¶„ì„",
            complexity="medium",
            requires_web_search=rule_based['web_search'],
            reasoning="LLM ë¶„ì„ ì‹¤íŒ¨ë¡œ ê·œì¹™ ê¸°ë°˜ ë¶„ì„ ì‚¬ìš©",
            metadata={'fallback': True}
        )

    def batch_analyze(self, questions: List[str]) -> List[QuestionAnalysis]:
        """ì—¬ëŸ¬ ì§ˆë¬¸ ì¼ê´„ ë¶„ì„"""
        results = []

        for i, question in enumerate(questions):
            try:
                logger.info(f"ì§ˆë¬¸ ë¶„ì„ ì¤‘ {i+1}/{len(questions)}: {question[:50]}...")
                analysis = self.analyze_question(question)
                results.append(analysis)
            except Exception as e:
                logger.error(f"ì§ˆë¬¸ {i+1} ë¶„ì„ ì‹¤íŒ¨: {e}")
                results.append(self._fallback_analysis(question))

        logger.info(f"âœ… ì¼ê´„ ì§ˆë¬¸ ë¶„ì„ ì™„ë£Œ: {len(results)}ê°œ")
        return results

    def get_domain_statistics(self, analyses: List[QuestionAnalysis]) -> Dict[str, Any]:
        """ë¶„ì„ ê²°ê³¼ í†µê³„"""
        if not analyses:
            return {}

        stats = {
            'total_questions': len(analyses),
            'question_types': {},
            'domain_frequency': {},
            'urgency_distribution': {},
            'avg_confidence': 0.0,
            'web_search_required': 0,
            'complexity_distribution': {}
        }

        # í†µê³„ ìˆ˜ì§‘
        total_confidence = 0.0

        for analysis in analyses:
            # ì§ˆë¬¸ ìœ í˜• ë¶„í¬
            qtype = analysis.question_type.value
            stats['question_types'][qtype] = stats['question_types'].get(qtype, 0) + 1

            # ë„ë©”ì¸ ë¹ˆë„
            for domain in analysis.primary_domains + analysis.secondary_domains:
                stats['domain_frequency'][domain] = stats['domain_frequency'].get(domain, 0) + 1

            # ê¸´ê¸‰ë„ ë¶„í¬
            urgency = analysis.urgency_level.value
            stats['urgency_distribution'][urgency] = stats['urgency_distribution'].get(urgency, 0) + 1

            # ë³µì¡ë„ ë¶„í¬
            complexity = analysis.complexity
            stats['complexity_distribution'][complexity] = stats['complexity_distribution'].get(complexity, 0) + 1

            # í™•ì‹ ë„ í•©ê³„
            total_confidence += analysis.confidence_score

            # ì›¹ ê²€ìƒ‰ í•„ìš” ìˆ˜
            if analysis.requires_web_search:
                stats['web_search_required'] += 1

        # í‰ê·  í™•ì‹ ë„
        stats['avg_confidence'] = total_confidence / len(analyses)

        return stats

    def suggest_improvements(self, analysis: QuestionAnalysis) -> List[str]:
        """ì§ˆë¬¸ ê°œì„  ì œì•ˆ"""
        suggestions = []

        if analysis.confidence_score < 0.6:
            suggestions.append("ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•´ë³´ì„¸ìš”")

        if analysis.question_type == QuestionType.UNCLEAR:
            suggestions.append("ì§ˆë¬¸ì˜ ëª©ì ì´ë‚˜ ì›í•˜ëŠ” ë‹µë³€ì„ ëª…í™•íˆ í•´ì£¼ì„¸ìš”")

        if not analysis.primary_domains:
            suggestions.append("ê´€ë ¨ ë¶„ì•¼ë‚˜ ì¹´í…Œê³ ë¦¬ë¥¼ ëª…ì‹œí•´ë³´ì„¸ìš”")

        if len(analysis.keywords) < 2:
            suggestions.append("í•µì‹¬ í‚¤ì›Œë“œë¥¼ ë” í¬í•¨í•´ë³´ì„¸ìš”")

        return suggestions


# í¸ì˜ í•¨ìˆ˜ë“¤
def analyze_question(question: str, llm: Optional[CoolStayLLM] = None) -> QuestionAnalysis:
    """ì§ˆë¬¸ ë¶„ì„ í¸ì˜ í•¨ìˆ˜"""
    analyzer = QuestionAnalyzer(llm)
    return analyzer.analyze_question(question)


def get_best_domains(question: str, max_domains: int = 2) -> List[str]:
    """ì§ˆë¬¸ì— ê°€ì¥ ì í•©í•œ ë„ë©”ì¸ ë°˜í™˜"""
    analysis = analyze_question(question)
    all_domains = analysis.primary_domains + analysis.secondary_domains
    return all_domains[:max_domains]


def needs_web_search(question: str) -> bool:
    """ì›¹ ê²€ìƒ‰ í•„ìš” ì—¬ë¶€ íŒë‹¨"""
    analysis = analyze_question(question)
    return analysis.requires_web_search


if __name__ == "__main__":
    # ì§ˆë¬¸ ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸
    print("ğŸ” CoolStay ì§ˆë¬¸ ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸")
    print("=" * 50)

    analyzer = QuestionAnalyzer()

    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤
    test_questions = [
        "ì—°ì°¨ íœ´ê°€ëŠ” ì–´ë–»ê²Œ ì‹ ì²­í•˜ë‚˜ìš”?",
        "React ì»´í¬ë„ŒíŠ¸ ê°œë°œ ê°€ì´ë“œë¼ì¸ì´ ìˆë‚˜ìš”?",
        "CI/CD íŒŒì´í”„ë¼ì¸ ì„¤ì • ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”",
        "ìµœì‹  AI ê°œë°œ ë™í–¥ì´ ê¶ê¸ˆí•©ë‹ˆë‹¤",
        "íšŒì‚¬ ì •ì±…ê³¼ ê¸°ìˆ  í‘œì¤€ì„ ëª¨ë‘ ì•Œê³  ì‹¶ì–´ìš”"
    ]

    print(f"ğŸ§ª {len(test_questions)}ê°œ ì§ˆë¬¸ ë¶„ì„ í…ŒìŠ¤íŠ¸:\n")

    analyses = []
    for i, question in enumerate(test_questions, 1):
        print(f"ğŸ“‹ ì§ˆë¬¸ {i}: {question}")

        try:
            analysis = analyzer.analyze_question(question)
            analyses.append(analysis)

            print(f"   ìœ í˜•: {analysis.question_type.value}")
            print(f"   ì£¼ ë„ë©”ì¸: {', '.join(analysis.primary_domains)}")
            print(f"   ë¶€ ë„ë©”ì¸: {', '.join(analysis.secondary_domains) if analysis.secondary_domains else 'ì—†ìŒ'}")
            print(f"   ì›¹ ê²€ìƒ‰: {'í•„ìš”' if analysis.requires_web_search else 'ë¶ˆí•„ìš”'}")
            print(f"   í™•ì‹ ë„: {analysis.confidence_score:.2f}")
            print(f"   ê¸´ê¸‰ë„: {analysis.urgency_level.value}")
            print(f"   ë³µì¡ë„: {analysis.complexity}")
            print(f"   í‚¤ì›Œë“œ: {', '.join(analysis.keywords[:3])}")

            # ê°œì„  ì œì•ˆ
            suggestions = analyzer.suggest_improvements(analysis)
            if suggestions:
                print(f"   ğŸ’¡ ê°œì„  ì œì•ˆ: {suggestions[0]}")

        except Exception as e:
            print(f"   âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")

        print()

    # í†µê³„ ë¶„ì„
    if analyses:
        print("ğŸ“Š ë¶„ì„ í†µê³„:")
        stats = analyzer.get_domain_statistics(analyses)

        print(f"   í‰ê·  í™•ì‹ ë„: {stats['avg_confidence']:.2f}")
        print(f"   ì›¹ ê²€ìƒ‰ í•„ìš”: {stats['web_search_required']}/{stats['total_questions']}ê°œ")

        print(f"   ë„ë©”ì¸ ë¹ˆë„:")
        for domain, count in sorted(stats['domain_frequency'].items(), key=lambda x: x[1], reverse=True):
            print(f"     - {domain}: {count}íšŒ")

        print(f"   ì§ˆë¬¸ ìœ í˜• ë¶„í¬:")
        for qtype, count in stats['question_types'].items():
            print(f"     - {qtype}: {count}ê°œ")
    else:
        print("âŒ ë¶„ì„ëœ ì§ˆë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")


# í¸ì˜ í•¨ìˆ˜ë“¤
def create_question_analyzer(config: Optional[CoolStayConfig] = None) -> QuestionAnalyzer:
    """ì§ˆë¬¸ ë¶„ì„ê¸°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    return QuestionAnalyzer(config)


def analyze_question_simple(
    question: str,
    config: Optional[CoolStayConfig] = None
) -> QuestionAnalysis:
    """ê°„ë‹¨í•œ ì§ˆë¬¸ ë¶„ì„ í•¨ìˆ˜"""
    analyzer = QuestionAnalyzer(config)
    return analyzer.analyze_question(question)