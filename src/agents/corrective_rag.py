"""
CoolStay RAG ì‹œìŠ¤í…œ Corrective RAG ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ í’ˆì§ˆ í‰ê°€ ê¸°ë°˜ ìê°€êµì • RAG ë©”ì»¤ë‹ˆì¦˜ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import logging
import time
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum

from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser

from ..core.llm import CoolStayLLM, get_default_llm
from .base_agent import BaseRAGAgent, AgentResponse, AgentStatus

logger = logging.getLogger(__name__)


class AnswerQuality(Enum):
    """ë‹µë³€ í’ˆì§ˆ ë“±ê¸‰"""
    EXCELLENT = "excellent"
    GOOD = "good"
    FAIR = "fair"
    POOR = "poor"


@dataclass
class QualityAssessment:
    """í’ˆì§ˆ í‰ê°€ ê²°ê³¼"""
    overall_quality: AnswerQuality
    relevance_score: float      # ê´€ë ¨ì„± (0-1)
    accuracy_score: float       # ì •í™•ì„± (0-1)
    completeness_score: float   # ì™„ì„±ë„ (0-1)
    confidence_score: float     # í™•ì‹ ë„ (0-1)
    reasoning: str              # í‰ê°€ ì´ìœ 
    needs_improvement: bool     # ê°œì„  í•„ìš” ì—¬ë¶€
    improvement_suggestions: List[str] = None  # ê°œì„  ì œì•ˆ


@dataclass
class CorrectiveResponse:
    """êµì • RAG ì‘ë‹µ"""
    final_answer: str
    source_documents: List[Document]
    domain: str
    agent_name: str
    quality_assessment: QualityAssessment
    iterations: int
    total_processing_time: float
    iteration_history: List[Dict[str, Any]]
    status: AgentStatus = AgentStatus.READY
    metadata: Optional[Dict[str, Any]] = None


class QualityEvaluator:
    """ë‹µë³€ í’ˆì§ˆ í‰ê°€ê¸°"""

    def __init__(self, llm: Optional[CoolStayLLM] = None):
        """
        í’ˆì§ˆ í‰ê°€ê¸° ì´ˆê¸°í™”

        Args:
            llm: ì‚¬ìš©í•  LLM ì¸ìŠ¤í„´ìŠ¤
        """
        self.llm = llm or get_default_llm()
        self._setup_evaluation_prompt()

    def _setup_evaluation_prompt(self):
        """í‰ê°€ í”„ë¡¬í”„íŠ¸ ì„¤ì •"""
        self.evaluation_prompt = ChatPromptTemplate.from_template("""
ë‹¹ì‹ ì€ RAG ì‹œìŠ¤í…œì˜ ë‹µë³€ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

**í‰ê°€ ê¸°ì¤€ (ê°ê° 0.0-1.0 ì ìˆ˜):**
1. **ê´€ë ¨ì„± (Relevance)**: ì§ˆë¬¸ê³¼ ë‹µë³€ì´ ì–¼ë§ˆë‚˜ ê´€ë ¨ì´ ìˆëŠ”ê°€?
2. **ì •í™•ì„± (Accuracy)**: ì œê³µëœ ì •ë³´ê°€ ì–¼ë§ˆë‚˜ ì •í™•í•œê°€?
3. **ì™„ì„±ë„ (Completeness)**: ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì´ ì–¼ë§ˆë‚˜ ì™„ì „í•œê°€?
4. **í™•ì‹ ë„ (Confidence)**: ë‹µë³€ì˜ ì‹ ë¢°ë„ëŠ” ì–¼ë§ˆë‚˜ ë†’ì€ê°€?

**ì§ˆë¬¸:** {question}

**ì œê³µëœ ì»¨í…ìŠ¤íŠ¸:**
{context}

**ìƒì„±ëœ ë‹µë³€:**
{answer}

**í‰ê°€ ì§€ì¹¨:**
- ê° ê¸°ì¤€ì— ëŒ€í•´ 0.0~1.0 ì ìˆ˜ë¥¼ ë§¤ê¸°ì„¸ìš” (1.0ì´ ìµœê³ )
- ì „ì²´ì ì¸ í’ˆì§ˆ ë“±ê¸‰ì„ ê²°ì •í•˜ì„¸ìš” (excellent/good/fair/poor)
- ê°œì„ ì´ í•„ìš”í•œì§€ íŒë‹¨í•˜ê³  ê·¸ ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”
- ê°œì„ ì´ í•„ìš”í•˜ë‹¤ë©´ êµ¬ì²´ì ì¸ ì œì•ˆì„ ì œê³µí•˜ì„¸ìš”

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
    "overall_quality": "excellent|good|fair|poor",
    "relevance_score": 0.0-1.0,
    "accuracy_score": 0.0-1.0,
    "completeness_score": 0.0-1.0,
    "confidence_score": 0.0-1.0,
    "reasoning": "í‰ê°€ ì´ìœ ë¥¼ ìƒì„¸íˆ ì„¤ëª…",
    "needs_improvement": true|false,
    "improvement_suggestions": ["ê°œì„  ì œì•ˆ 1", "ê°œì„  ì œì•ˆ 2", ...]
}}
""")

        self.evaluation_chain = (
            self.evaluation_prompt
            | self.llm.llm
            | JsonOutputParser()
        )

    def evaluate(self, question: str, context: List[str], answer: str) -> QualityAssessment:
        """ë‹µë³€ í’ˆì§ˆ í‰ê°€"""
        try:
            context_text = "\n\n".join(context) if context else "ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ"

            result = self.evaluation_chain.invoke({
                "question": question,
                "context": context_text,
                "answer": answer
            })

            return QualityAssessment(
                overall_quality=AnswerQuality(result["overall_quality"]),
                relevance_score=result["relevance_score"],
                accuracy_score=result["accuracy_score"],
                completeness_score=result["completeness_score"],
                confidence_score=result["confidence_score"],
                reasoning=result["reasoning"],
                needs_improvement=result["needs_improvement"],
                improvement_suggestions=result.get("improvement_suggestions", [])
            )

        except Exception as e:
            logger.error(f"í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return QualityAssessment(
                overall_quality=AnswerQuality.FAIR,
                relevance_score=0.5,
                accuracy_score=0.5,
                completeness_score=0.5,
                confidence_score=0.5,
                reasoning="í‰ê°€ ê³¼ì •ì—ì„œ ì˜¤ë¥˜ ë°œìƒ",
                needs_improvement=True,
                improvement_suggestions=["ì‹œìŠ¤í…œ ì˜¤ë¥˜ í•´ê²° í•„ìš”"]
            )


class QueryRewriter:
    """ì¿¼ë¦¬ ì¬ì‘ì„±ê¸°"""

    def __init__(self, llm: Optional[CoolStayLLM] = None):
        """
        ì¿¼ë¦¬ ì¬ì‘ì„±ê¸° ì´ˆê¸°í™”

        Args:
            llm: ì‚¬ìš©í•  LLM ì¸ìŠ¤í„´ìŠ¤
        """
        self.llm = llm or get_default_llm()
        self._setup_rewrite_prompt()

    def _setup_rewrite_prompt(self):
        """ì¬ì‘ì„± í”„ë¡¬í”„íŠ¸ ì„¤ì •"""
        self.rewrite_prompt = ChatPromptTemplate.from_template("""
ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ê°€ ë§Œì¡±ìŠ¤ëŸ½ì§€ ì•ŠìŠµë‹ˆë‹¤.
ë” ë‚˜ì€ ê²€ìƒ‰ì„ ìœ„í•´ ì§ˆë¬¸ì„ ê°œì„ í•´ì£¼ì„¸ìš”.

**ì›ë˜ ì§ˆë¬¸:** {original_question}

**ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸:**
{context}

**í’ˆì§ˆ í‰ê°€ í”¼ë“œë°±:**
{quality_feedback}

**ë„ë©”ì¸ ì •ë³´:** {domain_description}

**ê°œì„  ìš”ì²­:**
- ë” êµ¬ì²´ì ì´ê³  ëª…í™•í•œ ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ ê°œì„ 
- {domain_description} ë„ë©”ì¸ì— íŠ¹í™”ëœ ìš©ì–´ ì‚¬ìš©
- ì—¬ëŸ¬ ê´€ì ì—ì„œ ì ‘ê·¼í•  ìˆ˜ ìˆë„ë¡ í™•ì¥
- ê²€ìƒ‰ ì„±ëŠ¥ì„ ë†’ì¼ ìˆ˜ ìˆëŠ” í‚¤ì›Œë“œ ì¶”ê°€

**ê°œì„ ëœ ì§ˆë¬¸ (í•œêµ­ì–´ë¡œë§Œ):**
""")

        # LangChain pipeline í‘œí˜„ì‹ì„ í˜¸í™˜ì„±ì„ ìœ„í•´ í•¨ìˆ˜ í˜¸ì¶œë¡œ ë³€ê²½
        from langchain_core.runnables import RunnableLambda

        self.rewrite_chain = (
            self.rewrite_prompt
            | self.llm.llm
            | RunnableLambda(lambda x: x.content.strip())
        )

    def rewrite(self, original_question: str, context: List[str],
                quality_feedback: str, domain_description: str) -> str:
        """ì¿¼ë¦¬ ì¬ì‘ì„±"""
        try:
            context_text = "\n\n".join(context) if context else "ê´€ë ¨ ì •ë³´ ì—†ìŒ"

            improved_query = self.rewrite_chain.invoke({
                "original_question": original_question,
                "context": context_text,
                "quality_feedback": quality_feedback,
                "domain_description": domain_description
            })

            return improved_query.strip()

        except Exception as e:
            logger.error(f"ì¿¼ë¦¬ ì¬ì‘ì„± ì‹¤íŒ¨: {e}")
            return original_question


class CorrectiveRAGAgent(BaseRAGAgent):
    """êµì • RAG ì—ì´ì „íŠ¸"""

    def __init__(self, domain: str, llm: Optional[CoolStayLLM] = None,
                 chroma_manager=None, max_iterations: int = 3,
                 quality_threshold: float = 0.7):
        """
        êµì • RAG ì—ì´ì „íŠ¸ ì´ˆê¸°í™”

        Args:
            domain: ë‹´ë‹¹ ë„ë©”ì¸
            llm: LLM ì¸ìŠ¤í„´ìŠ¤
            chroma_manager: ChromaDB ê´€ë¦¬ì
            max_iterations: ìµœëŒ€ êµì • ë°˜ë³µ íšŸìˆ˜
            quality_threshold: í’ˆì§ˆ ì„ê³„ê°’ (ì´í•˜ë©´ ì¬ì‹œë„)
        """
        super().__init__(domain, llm, chroma_manager)

        self.max_iterations = max_iterations
        self.quality_threshold = quality_threshold

        # êµì • ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.quality_evaluator = QualityEvaluator(llm)
        self.query_rewriter = QueryRewriter(llm)

        logger.info(f"ğŸ”§ {domain} Corrective RAG ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")

    def corrective_query(self, question: str, **kwargs) -> CorrectiveResponse:
        """êµì • RAG ì§ˆë¬¸ ì²˜ë¦¬"""
        start_time = time.time()
        self.status = AgentStatus.BUSY

        iteration_history = []
        current_question = question
        best_response = None
        best_quality = None

        try:
            for iteration in range(self.max_iterations):
                iteration_start = time.time()
                logger.info(f"ğŸ”„ {self.domain} êµì • ë°˜ë³µ {iteration + 1}/{self.max_iterations}")

                # 1. ê¸°ë³¸ RAG ì²˜ë¦¬
                basic_response = super().process_query(current_question)

                # 2. í’ˆì§ˆ í‰ê°€
                context_texts = [doc.page_content for doc in basic_response.source_documents]
                quality = self.quality_evaluator.evaluate(
                    question, context_texts, basic_response.answer
                )

                iteration_time = time.time() - iteration_start

                # ë°˜ë³µ ê¸°ë¡
                iteration_record = {
                    'iteration': iteration + 1,
                    'question': current_question,
                    'answer': basic_response.answer,
                    'quality': quality,
                    'documents_count': len(basic_response.source_documents),
                    'processing_time': iteration_time,
                    'is_original_question': (iteration == 0)
                }
                iteration_history.append(iteration_record)

                # 3. í’ˆì§ˆ í™•ì¸
                avg_quality = (
                    quality.relevance_score + quality.accuracy_score +
                    quality.completeness_score + quality.confidence_score
                ) / 4

                # ìµœê³  í’ˆì§ˆ ì‘ë‹µ ì¶”ì 
                if best_quality is None or avg_quality > best_quality:
                    best_response = basic_response
                    best_quality = avg_quality

                # í’ˆì§ˆì´ ì¶©ì¡±ë˜ë©´ ì¢…ë£Œ
                if not quality.needs_improvement or avg_quality >= self.quality_threshold:
                    logger.info(f"âœ… {self.domain} í’ˆì§ˆ ëª©í‘œ ë‹¬ì„± (ì ìˆ˜: {avg_quality:.2f})")
                    break

                # ë§ˆì§€ë§‰ ë°˜ë³µì´ë©´ ì¢…ë£Œ
                if iteration >= self.max_iterations - 1:
                    logger.info(f"ğŸ”„ {self.domain} ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ ë„ë‹¬")
                    break

                # 4. ì¿¼ë¦¬ ì¬ì‘ì„±
                logger.info(f"ğŸ”§ {self.domain} ì¿¼ë¦¬ ì¬ì‘ì„± ì¤‘...")
                current_question = self.query_rewriter.rewrite(
                    question, context_texts, quality.reasoning, self.description
                )

                logger.info(f"   ì›ë˜ ì§ˆë¬¸: {question}")
                logger.info(f"   ê°œì„ ëœ ì§ˆë¬¸: {current_question}")

            total_time = time.time() - start_time

            # ìµœì¢… ì‘ë‹µ ìƒì„±
            final_response = CorrectiveResponse(
                final_answer=best_response.answer,
                source_documents=best_response.source_documents,
                domain=self.domain,
                agent_name=self.agent_name,
                quality_assessment=quality,
                iterations=len(iteration_history),
                total_processing_time=total_time,
                iteration_history=iteration_history,
                status=AgentStatus.READY,
                metadata={
                    'original_question': question,
                    'final_question': current_question,
                    'max_iterations': self.max_iterations,
                    'quality_threshold': self.quality_threshold,
                    'achieved_quality': best_quality,
                    'improvement_used': len(iteration_history) > 1
                }
            )

            self.status = AgentStatus.READY
            logger.info(f"âœ… {self.domain} êµì • RAG ì™„ë£Œ: {len(iteration_history)}íšŒ ë°˜ë³µ, í’ˆì§ˆ {best_quality:.2f}")

            return final_response

        except Exception as e:
            self.status = AgentStatus.ERROR
            error_msg = str(e)
            logger.error(f"âŒ {self.domain} êµì • RAG ì‹¤íŒ¨: {error_msg}")

            # ê¸°ë³¸ ì‘ë‹µ ë°˜í™˜ (ì˜¤ë¥˜ ë°œìƒ ì‹œ)
            fallback_response = super().process_query(question)

            return CorrectiveResponse(
                final_answer=f"êµì • ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ. ê¸°ë³¸ ë‹µë³€: {fallback_response.answer}",
                source_documents=fallback_response.source_documents,
                domain=self.domain,
                agent_name=self.agent_name,
                quality_assessment=QualityAssessment(
                    overall_quality=AnswerQuality.FAIR,
                    relevance_score=0.5,
                    accuracy_score=0.5,
                    completeness_score=0.5,
                    confidence_score=0.5,
                    reasoning=f"êµì • ì²˜ë¦¬ ì˜¤ë¥˜: {error_msg}",
                    needs_improvement=True
                ),
                iterations=len(iteration_history),
                total_processing_time=time.time() - start_time,
                iteration_history=iteration_history,
                status=AgentStatus.ERROR,
                metadata={'error': error_msg}
            )

    def process_query(self, question: str, enable_corrective: bool = True, **kwargs) -> CorrectiveResponse:
        """ì§ˆë¬¸ ì²˜ë¦¬ (êµì • ê¸°ëŠ¥ í¬í•¨)"""
        if enable_corrective:
            return self.corrective_query(question, **kwargs)
        else:
            # ê¸°ë³¸ RAGë§Œ ì‚¬ìš©
            basic_response = super().process_query(question, **kwargs)
            return CorrectiveResponse(
                final_answer=basic_response.answer,
                source_documents=basic_response.source_documents,
                domain=self.domain,
                agent_name=self.agent_name,
                quality_assessment=QualityAssessment(
                    overall_quality=AnswerQuality.GOOD,
                    relevance_score=0.8,
                    accuracy_score=0.8,
                    completeness_score=0.8,
                    confidence_score=0.8,
                    reasoning="ê¸°ë³¸ RAG ì‚¬ìš© (êµì • ë¹„í™œì„±í™”)",
                    needs_improvement=False
                ),
                iterations=1,
                total_processing_time=basic_response.processing_time or 0.0,
                iteration_history=[{
                    'iteration': 1,
                    'question': question,
                    'answer': basic_response.answer,
                    'quality': None,
                    'documents_count': len(basic_response.source_documents),
                    'processing_time': basic_response.processing_time,
                    'is_original_question': True
                }],
                status=basic_response.status,
                metadata={'corrective_disabled': True}
            )

    def get_corrective_stats(self) -> Dict[str, Any]:
        """êµì • í†µê³„ ë°˜í™˜"""
        base_status = self.get_status()

        corrective_stats = {
            **base_status,
            'corrective_features': {
                'max_iterations': self.max_iterations,
                'quality_threshold': self.quality_threshold,
                'quality_evaluator_ready': self.quality_evaluator is not None,
                'query_rewriter_ready': self.query_rewriter is not None
            }
        }

        return corrective_stats


# í¸ì˜ í•¨ìˆ˜ë“¤
def create_corrective_agent(domain: str, llm: Optional[CoolStayLLM] = None,
                           chroma_manager=None, max_iterations: int = 3) -> CorrectiveRAGAgent:
    """êµì • RAG ì—ì´ì „íŠ¸ ìƒì„± í¸ì˜ í•¨ìˆ˜"""
    return CorrectiveRAGAgent(domain, llm, chroma_manager, max_iterations)


def create_all_corrective_agents(llm: Optional[CoolStayLLM] = None,
                                chroma_manager=None) -> Dict[str, CorrectiveRAGAgent]:
    """ëª¨ë“  ë„ë©”ì¸ êµì • ì—ì´ì „íŠ¸ ìƒì„±"""
    from ..core.config import config

    agents = {}

    for domain in config.domain_list:
        try:
            agent = CorrectiveRAGAgent(domain, llm, chroma_manager)
            if agent.status != AgentStatus.ERROR:
                agents[domain] = agent
                logger.info(f"âœ… {domain} êµì • ì—ì´ì „íŠ¸ ìƒì„± ì™„ë£Œ")
            else:
                logger.warning(f"âš ï¸ {domain} êµì • ì—ì´ì „íŠ¸ ìƒì„± ì‹¤íŒ¨")
        except Exception as e:
            logger.error(f"âŒ {domain} êµì • ì—ì´ì „íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")

    logger.info(f"ğŸ‰ êµì • ì—ì´ì „íŠ¸ ìƒì„± ì™„ë£Œ: {len(agents)}/{len(config.domain_list)}ê°œ")
    return agents


if __name__ == "__main__":
    # êµì • RAG ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸
    print("ğŸ”§ CoolStay êµì • RAG ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸")
    print("=" * 50)

    # ë‹¨ì¼ êµì • ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸
    test_domain = "hr_policy"
    print(f"ğŸ” {test_domain} êµì • ì—ì´ì „íŠ¸ ìƒì„± ì¤‘...")

    agent = create_corrective_agent(test_domain, max_iterations=2)
    stats = agent.get_corrective_stats()

    print(f"ğŸ“Š êµì • ì—ì´ì „íŠ¸ ìƒíƒœ:")
    print(f"  ë„ë©”ì¸: {stats['domain']}")
    print(f"  ìƒíƒœ: {stats['status']}")
    print(f"  ìµœëŒ€ ë°˜ë³µ: {stats['corrective_features']['max_iterations']}")
    print(f"  í’ˆì§ˆ ì„ê³„ê°’: {stats['corrective_features']['quality_threshold']}")

    # í—¬ìŠ¤ ì²´í¬
    health = agent.health_check()
    print(f"\nğŸ¥ í—¬ìŠ¤ ì²´í¬: {health['overall_status']}")

    # êµì • ì§ˆë¬¸ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
    if health['overall_status'] in ['healthy', 'degraded']:
        print(f"\nğŸ’¬ êµì • RAG í…ŒìŠ¤íŠ¸:")
        test_question = "ì—°ì°¨ëŠ” ì–¸ì œ ì‚¬ìš©í•  ìˆ˜ ìˆë‚˜ìš”?"
        print(f"   ì§ˆë¬¸: {test_question}")

        # êµì • ê¸°ëŠ¥ í™œì„±í™”
        response = agent.process_query(test_question, enable_corrective=True)

        print(f"   ìµœì¢… ë‹µë³€: {response.final_answer[:200]}...")
        print(f"   ì´ ì²˜ë¦¬ ì‹œê°„: {response.total_processing_time:.2f}ì´ˆ")
        print(f"   ë°˜ë³µ íšŸìˆ˜: {response.iterations}")
        print(f"   í’ˆì§ˆ ì ìˆ˜: {(response.quality_assessment.relevance_score + response.quality_assessment.accuracy_score + response.quality_assessment.completeness_score + response.quality_assessment.confidence_score) / 4:.2f}")
        print(f"   ê°œì„  ì‚¬ìš©: {'ì˜ˆ' if response.metadata.get('improvement_used') else 'ì•„ë‹ˆì˜¤'}")

        # ë°˜ë³µ íˆìŠ¤í† ë¦¬
        print(f"\nğŸ“ˆ ë°˜ë³µ íˆìŠ¤í† ë¦¬:")
        for i, history in enumerate(response.iteration_history):
            print(f"   ë°˜ë³µ {history['iteration']}: {history['processing_time']:.2f}ì´ˆ, ë¬¸ì„œ {history['documents_count']}ê°œ")
    else:
        print(f"\nâŒ ì—ì´ì „íŠ¸ê°€ ì •ìƒ ìƒíƒœê°€ ì•„ë‹ˆì–´ì„œ í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")

    print(f"\nğŸ¯ ëª¨ë“  êµì • ì—ì´ì „íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸:")
    all_agents = create_all_corrective_agents()
    print(f"   ìƒì„±ëœ êµì • ì—ì´ì „íŠ¸: {len(all_agents)}ê°œ")