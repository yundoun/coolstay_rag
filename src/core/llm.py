"""
CoolStay RAG ì‹œìŠ¤í…œ LLM (ëŒ€ì–¸ì–´ëª¨ë¸) ê´€ë¦¬ ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ OpenAIì˜ ChatGPT ëª¨ë¸ì„ ê´€ë¦¬í•˜ê³  RAG ì‹œìŠ¤í…œì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ”
í†µí•©ëœ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
"""

import os
import logging
from typing import Optional, Dict, Any, List
from dataclasses import dataclass
import time

from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser

from .config import config, ModelConfig

logger = logging.getLogger(__name__)


@dataclass
class LLMResponse:
    """LLM ì‘ë‹µ ê²°ê³¼"""
    content: str
    model: str
    usage_tokens: Optional[int] = None
    response_time: Optional[float] = None
    error: Optional[str] = None
    success: bool = True


class CoolStayLLM:
    """CoolStay RAG ì‹œìŠ¤í…œìš© LLM ë˜í¼ í´ë˜ìŠ¤"""

    def __init__(self, model_config: Optional[ModelConfig] = None):
        """
        LLM ì´ˆê¸°í™”

        Args:
            model_config: ëª¨ë¸ ì„¤ì •. Noneì¸ ê²½ìš° ê¸°ë³¸ ì„¤ì • ì‚¬ìš©
        """
        self.config = model_config or config.openai_config
        self.llm: Optional[ChatOpenAI] = None
        self.is_initialized = False
        self.initialization_error = None

        # ì´ˆê¸°í™” ì‹œë„
        self.initialize()

    @property
    def model_name(self) -> str:
        """ëª¨ë¸ ì´ë¦„ ë°˜í™˜"""
        return self.config.name

    def initialize(self) -> bool:
        """LLM ì´ˆê¸°í™”"""
        try:
            # API í‚¤ ê²€ì¦
            if not self.config.api_key:
                raise ValueError("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

            if not self.config.api_key.startswith('sk-'):
                raise ValueError("ì˜¬ë°”ë¥´ì§€ ì•Šì€ OpenAI API í‚¤ í˜•ì‹ì…ë‹ˆë‹¤.")

            # ChatOpenAI ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            self.llm = ChatOpenAI(
                model=self.config.name,
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens,
                api_key=self.config.api_key
            )

            # ì—°ê²° í…ŒìŠ¤íŠ¸
            test_response = self._test_connection()
            if test_response.success:
                self.is_initialized = True
                logger.info(f"âœ… LLM ì´ˆê¸°í™” ì„±ê³µ: {self.config.name}")
                return True
            else:
                raise Exception(f"ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {test_response.error}")

        except Exception as e:
            self.initialization_error = str(e)
            self.is_initialized = False
            logger.error(f"âŒ LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

    def _test_connection(self) -> LLMResponse:
        """LLM ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            start_time = time.time()
            response = self.llm.invoke([HumanMessage(content="Hello")])
            response_time = time.time() - start_time

            return LLMResponse(
                content=response.content,
                model=self.config.name,
                response_time=response_time,
                success=True
            )

        except Exception as e:
            return LLMResponse(
                content="",
                model=self.config.name,
                error=str(e),
                success=False
            )

    def invoke(self, messages: List[Any]) -> LLMResponse:
        """ë©”ì‹œì§€ë¡œ LLM í˜¸ì¶œ"""
        if not self.is_initialized:
            return LLMResponse(
                content="",
                model=self.config.name,
                error=f"LLMì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {self.initialization_error}",
                success=False
            )

        try:
            start_time = time.time()
            response = self.llm.invoke(messages)
            response_time = time.time() - start_time

            return LLMResponse(
                content=response.content,
                model=self.config.name,
                response_time=response_time,
                success=True
            )

        except Exception as e:
            logger.error(f"LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return LLMResponse(
                content="",
                model=self.config.name,
                error=str(e),
                success=False
            )

    def invoke_with_prompt(self, prompt_template: str, **kwargs) -> LLMResponse:
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ìœ¼ë¡œ LLM í˜¸ì¶œ"""
        if not self.is_initialized:
            return LLMResponse(
                content="",
                model=self.config.name,
                error=f"LLMì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {self.initialization_error}",
                success=False
            )

        try:
            # í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…
            formatted_prompt = prompt_template.format(**kwargs)

            start_time = time.time()
            response = self.llm.invoke([HumanMessage(content=formatted_prompt)])
            response_time = time.time() - start_time

            return LLMResponse(
                content=response.content,
                model=self.config.name,
                response_time=response_time,
                success=True
            )

        except Exception as e:
            logger.error(f"í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return LLMResponse(
                content="",
                model=self.config.name,
                error=str(e),
                success=False
            )

    def get_chain_with_parser(self, prompt_template: ChatPromptTemplate, parser_type: str = "str"):
        """ì²´ì¸ê³¼ íŒŒì„œë¥¼ í•¨ê»˜ ë°˜í™˜"""
        if not self.is_initialized:
            raise RuntimeError(f"LLMì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {self.initialization_error}")

        if parser_type == "json":
            parser = JsonOutputParser()
        else:
            parser = StrOutputParser()

        return prompt_template | self.llm | parser

    def create_rag_answer_chain(self, agent_name: str, description: str) -> Any:
        """RAG ë‹µë³€ ìƒì„±ìš© ì²´ì¸ ìƒì„±"""
        if not self.is_initialized:
            raise RuntimeError(f"LLMì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {self.initialization_error}")

        prompt = ChatPromptTemplate.from_template("""
        ë‹¹ì‹ ì€ ê¿€ìŠ¤í…Œì´ì˜ {agent_name}ì…ë‹ˆë‹¤.
        ì „ë¬¸ ë¶„ì•¼: {description}

        **ì—­í• :**
        - {description} ê´€ë ¨ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ìƒì„¸í•œ ë‹µë³€ ì œê³µ
        - ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€
        - ë¶ˆí™•ì‹¤í•œ ì •ë³´ëŠ” ëª…ì‹œì ìœ¼ë¡œ í‘œí˜„

        **ì§ˆë¬¸:** {question}

        **ê´€ë ¨ ë¬¸ì„œ:**
        {context}

        **ë‹µë³€ ì§€ì¹¨:**
        1. ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
        2. ì œê³µëœ ë¬¸ì„œì˜ ì •ë³´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ í™œìš©í•˜ì„¸ìš”
        3. êµ¬ì²´ì ì¸ ì˜ˆì‹œë‚˜ ì ˆì°¨ê°€ ìˆë‹¤ë©´ í¬í•¨í•˜ì„¸ìš”
        4. í™•ì‹¤í•˜ì§€ ì•Šì€ ì •ë³´ëŠ” "ë¬¸ì„œì— ë”°ë¥´ë©´..." ë“±ìœ¼ë¡œ í‘œí˜„í•˜ì„¸ìš”
        5. ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”

        **ë‹µë³€:**
        """)

        return prompt.partial(agent_name=agent_name, description=description) | self.llm | StrOutputParser()

    def create_quality_evaluation_chain(self) -> Any:
        """í’ˆì§ˆ í‰ê°€ìš© ì²´ì¸ ìƒì„±"""
        if not self.is_initialized:
            raise RuntimeError(f"LLMì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {self.initialization_error}")

        prompt = ChatPromptTemplate.from_template("""
        ë‹¹ì‹ ì€ RAG ì‹œìŠ¤í…œì˜ ë‹µë³€ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

        **í‰ê°€ ê¸°ì¤€:**
        1. **ê´€ë ¨ì„±**: ì§ˆë¬¸ê³¼ ë‹µë³€ì´ ì–¼ë§ˆë‚˜ ê´€ë ¨ì´ ìˆëŠ”ê°€?
        2. **ì •í™•ì„±**: ì œê³µëœ ì •ë³´ê°€ ì–¼ë§ˆë‚˜ ì •í™•í•œê°€?
        3. **ì™„ì„±ë„**: ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì´ ì–¼ë§ˆë‚˜ ì™„ì „í•œê°€?
        4. **í™•ì‹ ë„**: ë‹µë³€ì˜ ì‹ ë¢°ë„ëŠ” ì–¼ë§ˆë‚˜ ë†’ì€ê°€?

        **ì§ˆë¬¸:** {question}

        **ì œê³µëœ ì»¨í…ìŠ¤íŠ¸:**
        {context}

        **ìƒì„±ëœ ë‹µë³€:**
        {answer}

        **í‰ê°€ ìš”ì²­:**
        ê° ê¸°ì¤€ì— ëŒ€í•´ 0.0~1.0 ì ìˆ˜ë¥¼ ë§¤ê¸°ê³ , ì „ì²´ì ì¸ í’ˆì§ˆ ë“±ê¸‰(excellent/good/fair/poor)ì„ ê²°ì •í•˜ì„¸ìš”.
        ê°œì„ ì´ í•„ìš”í•œì§€ íŒë‹¨í•˜ê³  ê·¸ ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.

        ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
        {{
            "overall_quality": "excellent|good|fair|poor",
            "relevance_score": 0.0-1.0,
            "accuracy_score": 0.0-1.0,
            "completeness_score": 0.0-1.0,
            "confidence_score": 0.0-1.0,
            "reasoning": "í‰ê°€ ì´ìœ ",
            "needs_improvement": true|false
        }}
        """)

        return prompt | self.llm | JsonOutputParser()

    def create_query_improvement_chain(self, description: str) -> Any:
        """ì¿¼ë¦¬ ê°œì„ ìš© ì²´ì¸ ìƒì„±"""
        if not self.is_initialized:
            raise RuntimeError(f"LLMì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {self.initialization_error}")

        prompt = ChatPromptTemplate.from_template("""
        ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ê°€ ë§Œì¡±ìŠ¤ëŸ½ì§€ ì•ŠìŠµë‹ˆë‹¤.
        ë” ë‚˜ì€ ê²€ìƒ‰ì„ ìœ„í•´ ì§ˆë¬¸ì„ ê°œì„ í•´ì£¼ì„¸ìš”.

        **ì›ë˜ ì§ˆë¬¸:** {original_question}
        **ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸:** {context}
        **í’ˆì§ˆ í‰ê°€:** {quality_feedback}

        **ìš”ì²­:**
        - ë” êµ¬ì²´ì ì´ê³  ëª…í™•í•œ ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ ê°œì„ 
        - {description} ë„ë©”ì¸ì— íŠ¹í™”ëœ ìš©ì–´ ì‚¬ìš©
        - ì—¬ëŸ¬ ê´€ì ì—ì„œ ì ‘ê·¼í•  ìˆ˜ ìˆë„ë¡ í™•ì¥

        **ê°œì„ ëœ ì§ˆë¬¸:**
        """)

        return prompt.partial(description=description) | self.llm | StrOutputParser()

    def create_web_search_answer_chain(self) -> Any:
        """ì›¹ ê²€ìƒ‰ ë‹µë³€ ìƒì„±ìš© ì²´ì¸ ìƒì„±"""
        if not self.is_initialized:
            raise RuntimeError(f"LLMì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {self.initialization_error}")

        prompt = ChatPromptTemplate.from_template("""
        ë‹¹ì‹ ì€ ì›¹ ê²€ìƒ‰ì„ í†µí•´ ìµœì‹  ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

        **ì§ˆë¬¸:** {question}

        **ì›¹ ê²€ìƒ‰ ê²°ê³¼:**
        {search_results}

        **ë‹µë³€ ì§€ì¹¨:**
        1. ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•œ ì •í™•í•œ ë‹µë³€ ì œê³µ
        2. ìµœì‹  ì •ë³´ì„ì„ ëª…ì‹œí•˜ê³  ì¶œì²˜ ì–¸ê¸‰
        3. ì—¬ëŸ¬ ì†ŒìŠ¤ì˜ ì •ë³´ê°€ ë‹¤ë¥¼ ê²½ìš° ì´ë¥¼ ëª…ì‹œ
        4. ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì •ë³´ ìš°ì„  ì‚¬ìš©
        5. ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±

        **ë‹µë³€:**
        """)

        return prompt | self.llm | StrOutputParser()

    def get_status(self) -> Dict[str, Any]:
        """LLM ìƒíƒœ ì •ë³´ ë°˜í™˜"""
        return {
            "initialized": self.is_initialized,
            "model": self.config.name,
            "temperature": self.config.temperature,
            "max_tokens": self.config.max_tokens,
            "api_key_set": bool(self.config.api_key),
            "initialization_error": self.initialization_error
        }


class LLMManager:
    """ì—¬ëŸ¬ LLM ì¸ìŠ¤í„´ìŠ¤ë¥¼ ê´€ë¦¬í•˜ëŠ” ë§¤ë‹ˆì € í´ë˜ìŠ¤"""

    def __init__(self):
        self.llm_instances: Dict[str, CoolStayLLM] = {}

    def get_llm(self, llm_type: str = "default") -> CoolStayLLM:
        """LLM ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì‹±ê¸€í†¤ íŒ¨í„´)"""
        if llm_type not in self.llm_instances:
            if llm_type == "default":
                self.llm_instances[llm_type] = CoolStayLLM()
            else:
                # ë‹¤ë¥¸ ì„¤ì •ì´ í•„ìš”í•œ ê²½ìš° í™•ì¥ ê°€ëŠ¥
                self.llm_instances[llm_type] = CoolStayLLM()

        return self.llm_instances[llm_type]

    def get_all_status(self) -> Dict[str, Dict[str, Any]]:
        """ëª¨ë“  LLM ì¸ìŠ¤í„´ìŠ¤ ìƒíƒœ ë°˜í™˜"""
        return {
            llm_type: llm.get_status()
            for llm_type, llm in self.llm_instances.items()
        }

    def test_all_connections(self) -> Dict[str, bool]:
        """ëª¨ë“  LLM ì—°ê²° í…ŒìŠ¤íŠ¸"""
        results = {}
        for llm_type, llm in self.llm_instances.items():
            if llm.is_initialized:
                test_response = llm._test_connection()
                results[llm_type] = test_response.success
            else:
                results[llm_type] = False
        return results


# ì „ì—­ LLM ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤
llm_manager = LLMManager()


# í¸ì˜ í•¨ìˆ˜ë“¤
def get_default_llm() -> CoolStayLLM:
    """ê¸°ë³¸ LLM ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    return llm_manager.get_llm("default")


def create_rag_chain(agent_name: str, description: str) -> Any:
    """RAG ì²´ì¸ ìƒì„± í¸ì˜ í•¨ìˆ˜"""
    llm = get_default_llm()
    return llm.create_rag_answer_chain(agent_name, description)


def create_quality_evaluator() -> Any:
    """í’ˆì§ˆ í‰ê°€ ì²´ì¸ ìƒì„± í¸ì˜ í•¨ìˆ˜"""
    llm = get_default_llm()
    return llm.create_quality_evaluation_chain()


def test_llm_connection() -> bool:
    """LLM ì—°ê²° í…ŒìŠ¤íŠ¸ í¸ì˜ í•¨ìˆ˜"""
    llm = get_default_llm()
    if llm.is_initialized:
        test_response = llm._test_connection()
        return test_response.success
    return False


if __name__ == "__main__":
    # LLM í…ŒìŠ¤íŠ¸
    print("ğŸ¤– CoolStay LLM ëª¨ë“ˆ í…ŒìŠ¤íŠ¸")
    print("=" * 50)

    # ê¸°ë³¸ LLM ê°€ì ¸ì˜¤ê¸°
    llm = get_default_llm()
    status = llm.get_status()

    print(f"ğŸ“Š LLM ìƒíƒœ:")
    for key, value in status.items():
        print(f"  {key}: {value}")

    # ì—°ê²° í…ŒìŠ¤íŠ¸
    if llm.is_initialized:
        print(f"\nğŸ” ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘...")
        test_response = llm._test_connection()

        if test_response.success:
            print(f"âœ… ì—°ê²° ì„±ê³µ!")
            print(f"   - ì‘ë‹µ ì‹œê°„: {test_response.response_time:.2f}ì´ˆ")
            print(f"   - ì‘ë‹µ ë‚´ìš©: {test_response.content}")
        else:
            print(f"âŒ ì—°ê²° ì‹¤íŒ¨: {test_response.error}")
    else:
        print(f"\nâŒ LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {status['initialization_error']}")